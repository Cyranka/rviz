if(i%% 3 == 0|i%%5 ==0){
my_sum <- my_sum + i
}else{
next
}
}
sum(1:999)
remove(list = ls())
source("/Users/harrocyranka/Desktop/code/twitter_henryads/twitter_info_analysis_3.R")
getCurRateLimitInfo()
library(rtweet)
x <- search_tweets(q = "AmericanHumane")
View(x)
remove(list = ls())
source("/Users/harrocyranka/Desktop/code/twitter_henryads/twitter_info_analysis_3.R")
library(rtweet)
x <- get_ids("thefortunesoc")
y <- get_users_api_direct(x,0)
readr::write_csv(y, "/Users/harrocyranka/Desktop/Research_and_Projects/fortune_society/")
readr::write_csv(y, "/Users/harrocyranka/Desktop/Research_and_Projects/fortune_society/fortune_soc_api.csv")
sample(c("Nathan", "Harro", "Mike", "Aly"),1)
sample(c("Nathan", "Harro", "Aly"),1)
sample(c(Harro", "Aly"),1)
sample(c("Harro", "Aly"),1)
sapply(1:50, function(i)sample(c("Max","Kyle","Nathan","Ethan","Josh","Mike","Harro"),1))
lapply(1:50, function(i)sample(c("Max","Kyle","Nathan","Ethan","Josh","Mike","Harro"),1))
lapply(1:50, function(i)sample(c("Max","Kyle","Nathan","Ethan","Josh","Mike","Harro"),1))
lapply(1:50, function(i)sample(c("Max","Kyle","Nathan","Ethan","Josh","Mike","Harro"),1))
lapply(1:50, function(i)sample(c("Max","Kyle","Nathan","Ethan","Josh","Mike","Harro"),1))
sapply(1:50, function(i)sample(c("Max","Kyle","Nathan","Ethan","Josh","Mike","Harro"),1))
lapply(list, function(i)sample(c("Ethan", "Nathan", "Harro","Kyle")))
lapply(1:100, function(i)sample(c("Ethan", "Nathan", "Harro","Kyle")))
lapply(1:100, function(i)sample(c("Ethan", "Nathan", "Harro","Kyle"),1))
lapply(1:100, function(i)sample(c("Ethan", "Nathan", "Harro","Kyle"),1))
lapply(1:100, function(i)sample(c("Ethan", "Nathan", "Mike","Kyle"),1))
lapply(1:100, function(i)sample(c("Ethan", "Harro", "Mike","Kyle"),1))
lapply(1:100, function(i)sample(c("Ethan", "Harro", "Mike","Kyle"),1))
sample(c(1:7),1)
sample(c(1:7),1)
rpois(1000)
rpois(10000, 2)
l <- rpois(10000, 2)
l <- lapply(1:10000, function(i) rpois(10000,4))
k <- sum(l)
k <- sum(unlist(l))
library(tidyverse)
l <- lapply(1:10000, function(i) rpois(10000,4))
k <- bind_rows(l)
l2 <- lapply(l, as_tibble)
l3 <- bind_cols(l2)
View(3)
View(l3)
k4 <- rowSums(l3)
hist(k4)
hist(l2[1,])
hist(l3[1,])
l3[1,]
hist(l3[,1])
l3[,1]
unlist(l3[1,])
unname(unlist(l3[1,]))
hist(unname(unlist(l3[1,])))
hist(unname(unlist(l3[,1])))
hist(unname(unlist(sum(l3[,1])))
)
hist(k4)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
source("/Users/harrocyranka/Desktop/code/twitter_info_analysis_3.R")
source("/Users/harrocyranka/Desktop/code/script_to_load_packages.R")
library(rtweet)
?get_timeline
x <- get_timeline("harrocyranka")
View(x)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
library(tidyverse)
read_and_sample <- function(file, sample_size, out_file){
x <- readr::read_csv(file) %>% sample_n(sample_size)
readr::write_csv(out_file)
return(paste0("File written as", out_file))
}
x <- readline("Enter file name to sample")
z <- readline("Enter sampled file to save")
read_and_sample(x, y, z)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/")
library(tidyverse);library(tidytext)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/test_reproduce/")
library(tidyverse);library(tidytext)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/test_reproduce/")
library(tidyverse);library(tidytext)
x <- read_csv("hadley_api.csv")
z <- read_excel("most_common_words_english.xlsx")
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/test_reproduce/")
library(tidyverse);library(tidytext)
x <- read_csv("hadley_api.csv")
z <- readxl::read_excel("most_common_words_english.xlsx")
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_bios <- x %>% select(screenName,text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "regex",pattern = unnest_reg) %>% filter(!word %in% stop_words$word,
str_detect(word,"[a-z]")) %>%
filter(!word %in% z$word)
top_unigrams <- tidy_bios %>% dplyr::count(word, sort = TRUE) ##Top Unigrams
top_unigrams
tidy_bios <- x %>% select(screenName,text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "regex",pattern = unnest_reg) %>% filter(!word %in% stop_words$word,
str_detect(word,"[a-z]")) %>%
filter(!word %in% z$word)
top_unigrams <- tidy_bios %>% dplyr::count(word, sort = TRUE) ##Top Unigrams
top_unigrams
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/test_reproduce/")
library(tidyverse);library(tidytext)
x <- read_csv("hadley_api.csv") %>% dplyr::rename(text = description)
z <- readxl::read_excel("most_common_words_english.xlsx")
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_bios <- x %>% select(screenName,text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "regex",pattern = unnest_reg) %>% filter(!word %in% stop_words$word,
str_detect(word,"[a-z]")) %>%
filter(!word %in% z$word)
top_unigrams <- tidy_bios %>% dplyr::count(word, sort = TRUE) ##Top Unigrams
top_unigrams
plot(tidy_bios %>% group_by(word) %>% dplyr::count(word, sort = TRUE) %>% ungroup() %>% slice(1:20) %>% mutate(word = str_to_title(word)) %>%ggplot(aes(reorder(word, n), n)) + geom_col(show.legend = FALSE, fill = "#018270") +
coord_flip() + labs(y = "Count", x = "Word",
title = "Most common unigrams in @hadleywickham's twitter followers bios") + theme_bw() + theme(text = element_text(size=14)))
tidy_bigrams <- x %>% select(screenName, text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "ngrams",n = 2)
count_bigrams <- tidy_bigrams %>% group_by(word) %>% tally() %>% arrange(desc(n)) %>% ungroup() %>% separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
filter(!word1 %in% z$word, !word2 %in% z$word) %>%
filter(!is.na(word1),!is.na(word2))
count_bigrams
plot(count_bigrams %>% unite("word", c("word1", "word2"), sep = " ") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Words",
title = "Most common bigrams in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
library(widyr)
bio_word_pairs <- tidy_bios %>% pairwise_count(word, screenName, sort = TRUE)
bio_word_pairs <- bio_word_pairs %>% mutate(to_filter = as.numeric(row.names(bio_word_pairs)) %%2) %>% filter(to_filter == 0) %>% mutate(to_filter =NULL)
plot(bio_word_pairs %>% unite("word", c("item1", "item2"), sep = "/") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = my_color) + theme_bw() + coord_flip() + labs(y = "Count", x = "Word Pairs",title = "Most common word pairs in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
plot(bio_word_pairs %>% unite("word", c("item1", "item2"), sep = "/") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Word Pairs",title = "Most common word pairs in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
plot(bio_word_pairs %>% unite("word", c("item1", "item2"), sep = "/") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Word Pairs",title = "Most common word pairs in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
##Sample 'good' bios for clustering
nice_bios <- x %>% filter(lang == "en" & text != "")
set.seed(144)
nice_bios <-nice_bios[sample(1:nrow(nice_bios),1500),]
readr::write_csv(nice_bios, "bios_for_clustering.csv")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/test_reproduce/")
library(tidyverse);library(tidytext)
x <- read_csv("hadley_api.csv") %>% dplyr::rename(text = description)
z <- readxl::read_excel("most_common_words_english.xlsx")
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
##Create tidy bios and get top unigrams
tidy_bios <- x %>% select(screenName,text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "regex",pattern = unnest_reg) %>% filter(!word %in% stop_words$word,
str_detect(word,"[a-z]")) %>%
filter(!word %in% z$word)
top_unigrams <- tidy_bios %>% dplyr::count(word, sort = TRUE) ##Top Unigrams
top_unigrams
###Create Unigram Plot
png(filename="one_word_twitter.png",width=800, height=500)
plot(tidy_bios %>% group_by(word) %>% dplyr::count(word, sort = TRUE) %>% ungroup() %>% slice(1:20) %>% mutate(word = str_to_title(word)) %>%ggplot(aes(reorder(word, n), n)) + geom_col(show.legend = FALSE, fill = "#018270") +
coord_flip() + labs(y = "Count", x = "Word",
title = "Most common unigrams in @hadleywickham's twitter followers bios") + theme_bw() + theme(text = element_text(size=14)))
dev.off()
##Create tidy bigrams and get plot
##Two Words
tidy_bigrams <- x %>% select(screenName, text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "ngrams",n = 2)
count_bigrams <- tidy_bigrams %>% group_by(word) %>% tally() %>% arrange(desc(n)) %>% ungroup() %>% separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
filter(!word1 %in% z$word, !word2 %in% z$word) %>%
filter(!is.na(word1),!is.na(word2))
png(filename="two_words_twitter.png",width=800, height=500)
plot(count_bigrams %>% unite("word", c("word1", "word2"), sep = " ") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Words",
title = "Most common bigrams in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
dev.off()
##
library(widyr)
bio_word_pairs <- tidy_bios %>% pairwise_count(word, screenName, sort = TRUE)
bio_word_pairs <- bio_word_pairs %>% mutate(to_filter = as.numeric(row.names(bio_word_pairs)) %%2) %>% filter(to_filter == 0) %>% mutate(to_filter =NULL)
png(filename="word_pairs_twitter.png",width=800, height=500)
plot(bio_word_pairs %>% unite("word", c("item1", "item2"), sep = "/") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Word Pairs",title = "Most common word pairs in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
saveRDS(bio_word_pairs, "bio_word_pairs.RDS")
##Sample 'good' bios for clustering
nice_bios <- x %>% filter(lang == "en" & text != "")
set.seed(144)
nice_bios <-nice_bios[sample(1:nrow(nice_bios),1500),]
readr::write_csv(nice_bios, "bios_for_clustering.csv")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/Research_and_Projects/ad_hoc/hadley_twitter/test_reproduce/")
library(tidyverse);library(tidytext)
x <- read_csv("hadley_api.csv") %>% dplyr::rename(text = description)
z <- readxl::read_excel("most_common_words_english.xlsx")
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
##Create tidy bios and get top unigrams
tidy_bios <- x %>% select(screenName,text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "regex",pattern = unnest_reg) %>% filter(!word %in% stop_words$word,
str_detect(word,"[a-z]")) %>%
filter(!word %in% z$word)
top_unigrams <- tidy_bios %>% dplyr::count(word, sort = TRUE) ##Top Unigrams
top_unigrams
###Create Unigram Plot
png(filename="one_word_twitter.png",width=800, height=500)
plot(tidy_bios %>% group_by(word) %>% dplyr::count(word, sort = TRUE) %>% ungroup() %>% slice(1:20) %>% mutate(word = str_to_title(word)) %>%ggplot(aes(reorder(word, n), n)) + geom_col(show.legend = FALSE, fill = "#018270") +
coord_flip() + labs(y = "Count", x = "Word",
title = "Most common unigrams in @hadleywickham's twitter followers bios") + theme_bw() + theme(text = element_text(size=14)))
dev.off()
##Create tidy bigrams and get plot
##Two Words
tidy_bigrams <- x %>% select(screenName, text) %>% mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "ngrams",n = 2)
count_bigrams <- tidy_bigrams %>% group_by(word) %>% tally() %>% arrange(desc(n)) %>% ungroup() %>% separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
filter(!word1 %in% z$word, !word2 %in% z$word) %>%
filter(!is.na(word1),!is.na(word2))
png(filename="two_words_twitter.png",width=800, height=500)
plot(count_bigrams %>% unite("word", c("word1", "word2"), sep = " ") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Words",
title = "Most common bigrams in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
dev.off()
##
library(widyr)
bio_word_pairs <- tidy_bios %>% pairwise_count(word, screenName, sort = TRUE)
bio_word_pairs <- bio_word_pairs %>% mutate(to_filter = as.numeric(row.names(bio_word_pairs)) %%2) %>% filter(to_filter == 0) %>% mutate(to_filter =NULL)
png(filename="word_pairs_twitter.png",width=800, height=500)
plot(bio_word_pairs %>% unite("word", c("item1", "item2"), sep = "/") %>% slice(1:20) %>% mutate(word  = str_to_title(word)) %>%
ggplot(aes(reorder(word,n),n)) +  geom_col(show.legend = FALSE, fill = "#018270") + theme_bw() + coord_flip() + labs(y = "Count", x = "Word Pairs",title = "Most common word pairs in @hadleywickham's twitter followers bios") + theme(text = element_text(size=14)))
dev.off()
saveRDS(bio_word_pairs, "bio_word_pairs.RDS")
##Sample 'good' bios for clustering
nice_bios <- x %>% filter(lang == "en" & text != "")
set.seed(144)
nice_bios <-nice_bios[sample(1:nrow(nice_bios),1500),]
readr::write_csv(nice_bios, "bios_for_clustering.csv")
remove(list = ls())
dev.off9
dev.off()
cities_twitter_2 <- function(data_frame){
library(scales)
cities <- readxl::read_excel("us_cities_database_with_state.xlsx")
y <- data_frame
city_number <- function(k){
l <- grep(pattern = tolower(cities$city_ascii[k]),tolower(y$location))
l2 <- length(l)
print(k)
return(l2)
}
count <- sapply(1:1000, city_number)
locations_top <- as.data.frame(cbind(cities[1:1000,],count))
locations_top <- arrange(locations_top, desc(count))
locations_top <- filter(locations_top, count > 0)
locations_top <- locations_top %>% select(city_ascii, lat, lng, population_proper, count) %>%
arrange(desc(count), desc(population_proper)) %>%
mutate(duplicates = duplicated(city_ascii)) %>%
filter(duplicates == FALSE) %>% select(-duplicates) %>%
dplyr::rename(lon = lng)
locations_top$percent <- round(locations_top$count/sum(locations_top$count),4)
locations_top$percent <- percent(locations_top$percent)
locations_top <- locations_top %>% dplyr::select(city_ascii,population_proper, lat, lon, count, percent) %>%
magrittr::set_colnames(c("name", "pop", "lat","lon", "count", "percent")) %>% as_tibble()
return(locations_top)
}
cities_twitter_2 <- function(data_frame){
library(scales)
cities <- readxl::read_excel("us_cities_database_with_state.xlsx")
y <- data_frame
city_number <- function(k){
l <- grep(pattern = tolower(cities$city_ascii[k]),tolower(y$location))
l2 <- length(l)
print(k)
return(l2)
}
count <- sapply(1:1000, city_number)
locations_top <- as.data.frame(cbind(cities[1:1000,],count))
locations_top <- arrange(locations_top, desc(count))
locations_top <- filter(locations_top, count > 0)
locations_top <- locations_top %>% select(city_ascii, lat, lng, population_proper, count) %>%
arrange(desc(count), desc(population_proper)) %>%
mutate(duplicates = duplicated(city_ascii)) %>%
filter(duplicates == FALSE) %>% select(-duplicates) %>%
dplyr::rename(lon = lng)
locations_top$percent <- round(locations_top$count/sum(locations_top$count),4)
locations_top$percent <- percent(locations_top$percent)
locations_top <- locations_top %>% dplyr::select(city_ascii,population_proper, lat, lon, count, percent) %>%
magrittr::set_colnames(c("name", "pop", "lat","lon", "count", "percent")) %>% as_tibble()
return(locations_top)
}
x <- read_csv("hadley_api.csv")
loc <- cities_twitter_2(x)
loc %>% select(name, count, percent) %>% write_csv("locations_for_table.csv")
loc %>% write_csv("locations_for_map.csv")
create_twitter_city_map <- function(csv_file, define_title,define_color, latitude_column= "lat", longitude_column ="lon", size_column = "count"){
library(plotly)
library(shiny)
##Bubble Map##
df2 <- readr::read_csv(csv_file)
g <- list(
scope = 'usa',
projection = list(type = 'albers usa'),
showland = TRUE,
landcolor = toRGB("white"),
subunitwidth = 1,
countrywidth = 1,
subunitcolor = toRGB("black"),
countrycolor = toRGB("black")
)
p <- plot_geo(df2, locationmode = 'USA-states', sizes = c(100, 5000)) %>% add_markers(
x = ~lon, y = ~lat, size = ~count,color = ~count,colors = c(define_color),hoverinfo = "text",
text = ~paste(df2$name, "<br />", df2$count/1e6, " million")
)  %>% layout(title = define_title, geo = g,showlegend = FALSE, width = 2000,height = 1000) %>%
hide_colorbar()
p2 <<- p
}
p2 <- create_twitter_city_map("locations_for_map.csv", "Matched US locations map for @hadleywickham's twitter followers",define_color = "#018270")
htmlwidgets::saveWidget(p2, "twitter_city_map.html", selfcontained = FALSE)
webshot::webshot("twitter_city_map.html", file = "newplot.png")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
library(tidyverse)
library(tidytext)
library(widyr)
#Word Pairs Network
library(igraph)
library(ggraph)
word_network <- function(my_number){
set.seed(123)
bio_word_pairs <- readRDS("bio_word_pairs.RDS")
the_number <- as.numeric(as.character(my_number))
define_color <- readr::read_csv("chosen_color.csv") %>% pull(color)
png(filename="word_network_twitter.png",width=591, height=422)
plot(bio_word_pairs %>%
filter(n >= the_number) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = define_color) +
geom_node_point(size = 3) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void())
dev.off()
}
define_number <- readline(200)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
library(tidyverse)
library(tidytext)
library(widyr)
#Word Pairs Network
library(igraph)
library(ggraph)
word_network <- function(my_number){
set.seed(123)
bio_word_pairs <- readRDS("bio_word_pairs.RDS")
the_number <- as.numeric(as.character(my_number))
define_color <- readr::read_csv("chosen_color.csv") %>% pull(color)
png(filename="word_network_twitter.png",width=591, height=422)
plot(bio_word_pairs %>%
filter(n >= the_number) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = define_color) +
geom_node_point(size = 3) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void())
dev.off()
}
define_number <- 200
word_network(define_number)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
library(tidyverse)
library(tidytext)
library(widyr)
#Word Pairs Network
library(igraph)
library(ggraph)
word_network <- function(my_number){
set.seed(123)
bio_word_pairs <- readRDS("bio_word_pairs.RDS")
the_number <- as.numeric(as.character(my_number))
define_color <- "#018270"
png(filename="word_network_twitter.png",width=591, height=422)
plot(bio_word_pairs %>%
filter(n >= the_number) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = define_color) +
geom_node_point(size = 3) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void())
dev.off()
}
define_number <- 200
word_network(define_number)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/rviz/tidytuesday_week_19/")
library(tidyverse)
x <- read_csv("tidy_tuesday_week_19.csv")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/rviz/tidytuesday_week_19/")
library(tidyverse)
x <- read_csv("tidy_tuesday_week_19.csv")
y <- read_csv("airline_by_continent.xlsx")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/rviz/tidytuesday_week_19/")
library(tidyverse)
x <- read_csv("tidy_tuesday_week_19.csv")
y <- readxl::read_excel("airline_by_continent.xlsx")
z <- readxl::read_excel("per_capita_income.xls")
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/rviz/tidytuesday_week_19/")
library(tidyverse)
x <- read_csv("tidy_tuesday_week_19.csv")
y <- readxl::read_excel("airline_by_continent.xlsx")
z <- readxl::read_excel("per_capita_income.xls")
View(z)
y
z <- readxl::read_excel("per_capita_income.xls") %>%
select(`Country Name`,`1985`:`2014`) %>%
filter(`Country Name` %in% y$country)
View(z)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/rviz/tidytuesday_week_19/")
library(tidyverse)
x <- read_csv("tidy_tuesday_week_19.csv")
y <- readxl::read_excel("airline_by_continent.xlsx")
z <- readxl::read_excel("per_capita_income.xls") %>%
select(`Country Name`,`1985`:`2014`) %>%
filter(`Country Name` %in% y$country)
remove(list = ls())
options(stringsAsFactors = FALSE)
options(scipen = 999)
setwd("/Users/harrocyranka/Desktop/rviz/tidytuesday_week_19/")
library(tidyverse)
x <- read_csv("tidy_tuesday_week_19.csv")
y <- readxl::read_excel("airline_by_continent.xlsx")
z <- readxl::read_excel("per_capita_income.xls") %>%
select(`Country Name`,`1985`:`2014`) %>%
filter(`Country Name` %in% y$country)
?gather
z %>% gather(year, per_capita_income)
z %>% gather(year, per_capita_income, -country)
z %>% gather(year, per_capita_income, -`Country Name`)
z %>% gather(year, per_capita_income, -`Country Name`) %>% rename(country = `Country Name`)
z %>% gather(year, per_capita_income, -`Country Name`) %>% rename(country = `Country Name`) %>%
mutate(period = ifelse(year <2000, "85-99", "00-14"))
z %>% gather(year, per_capita_income, -`Country Name`) %>% rename(country = `Country Name`) %>%
mutate(period = ifelse(year <2000, "85-99", "00-14")) %>%
group_by(country,period) %>% summarise(avg_pci = mean(per_capita_income, na.rm = TRUE))
z %>% gather(year, per_capita_income, -`Country Name`) %>% rename(country = `Country Name`) %>%
mutate(period = ifelse(year <2000, " 85-99", "00-14")) %>%
group_by(country,period) %>% summarise(avg_pci = mean(per_capita_income, na.rm = TRUE))
z %>% gather(year, per_capita_income, -`Country Name`) %>% rename(country = `Country Name`) %>%
mutate(period = ifelse(year <2000, "1985-99", "2000-14")) %>%
group_by(country,period) %>% summarise(avg_pci = mean(per_capita_income, na.rm = TRUE))
aggregate_pci <- z %>% gather(year, per_capita_income, -`Country Name`) %>% rename(country = `Country Name`) %>%
mutate(period = ifelse(year <2000, "1985-99", "2000-14")) %>%
group_by(country,period) %>% summarise(avg_pci = mean(per_capita_income, na.rm = TRUE))
